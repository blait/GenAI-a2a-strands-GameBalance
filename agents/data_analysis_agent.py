#!/usr/bin/env python3
from strands import Agent, tool
from strands.multiagent.a2a import A2AServer
from strands.models.bedrock import BedrockModel
from fastapi import FastAPI
from pydantic import BaseModel
import pandas as pd
import uvicorn
import asyncio

GAME_DATA = [
    {"game_id": 1, "race": "Terran", "result": "win", "duration": 15},
    {"game_id": 2, "race": "Zerg", "result": "loss", "duration": 20},
    {"game_id": 3, "race": "Protoss", "result": "loss", "duration": 18},
    {"game_id": 4, "race": "Terran", "result": "win", "duration": 12},
    {"game_id": 5, "race": "Terran", "result": "win", "duration": 14},
    {"game_id": 6, "race": "Zerg", "result": "loss", "duration": 22},
    {"game_id": 7, "race": "Protoss", "result": "win", "duration": 25},
    {"game_id": 8, "race": "Terran", "result": "win", "duration": 11},
    {"game_id": 9, "race": "Terran", "result": "win", "duration": 13},
    {"game_id": 10, "race": "Zerg", "result": "loss", "duration": 19},
] * 3

@tool
def analyze_win_rates(race: str = None) -> str:
    """Analyze win rates by race
    
    Args:
        race: Filter by specific race (Terran, Zerg, Protoss)
    """
    df = pd.DataFrame(GAME_DATA)
    if race:
        df = df[df["race"] == race]
    
    stats = df.groupby("race").apply(
        lambda x: f"{x['race'].iloc[0]}: {(x['result'] == 'win').sum() / len(x) * 100:.2f}% ({(x['result'] == 'win').sum()}/{len(x)})"
    )
    return "\n".join(stats.values)

@tool
def analyze_game_duration(race: str = None) -> str:
    """Analyze average game duration
    
    Args:
        race: Filter by specific race
    """
    df = pd.DataFrame(GAME_DATA)
    if race:
        df = df[df["race"] == race]
    
    stats = df.groupby("race")["duration"].mean()
    return "\n".join([f"{r}: {d:.1f}ë¶„" for r, d in stats.items()])

app = FastAPI()

class QueryRequest(BaseModel):
    query: str

agent = Agent(
    name="Data Analysis Agent",
    description="ê²Œì„ í†µê³„ì™€ ìŠ¹ë¥ ì„ ë¶„ì„í•˜ëŠ” ì—ì´ì „íŠ¸",
    model=BedrockModel(model_id="us.amazon.nova-lite-v1:0", temperature=0.3),
    tools=[analyze_win_rates, analyze_game_duration],
    system_prompt="""ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²Œì„ í†µê³„ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
- analyze_win_rates: ì¢…ì¡±ë³„ ìŠ¹ë¥  ë¶„ì„
- analyze_game_duration: í‰ê·  ê²Œì„ ì‹œê°„ ë¶„ì„

ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì—¬ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”.

**ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ ì‘ì„±í•˜ì„¸ìš”.**"""
)

@app.post("/ask")
async def ask(request: QueryRequest):
    result = await asyncio.to_thread(lambda: agent(request.query))
    return {"query": request.query, "response": result}

@app.post("/ask_stream")
async def ask_stream(request: QueryRequest):
    """Streaming endpoint for real-time thinking"""
    from fastapi.responses import StreamingResponse
    import json
    import queue
    import threading
    import sys
    
    event_queue = queue.Queue()
    
    class StreamCapture:
        def __init__(self, queue, original):
            self.queue = queue
            self.original = original
            
        def write(self, text):
            self.original.write(text)
            self.original.flush()
            if text and text.strip():
                self.queue.put(('log', text))
                
        def flush(self):
            self.original.flush()
    
    def run_agent():
        try:
            old_stdout = sys.stdout
            sys.stdout = StreamCapture(event_queue, old_stdout)
            
            print(f"\nğŸ¯ Query: {request.query}\n")
            
            result = agent(request.query)
            
            sys.stdout = old_stdout
            
            if hasattr(result, 'message') and hasattr(result.message, 'content'):
                content = result.message.content[0].text if result.message.content else ""
            else:
                content = str(result)
            
            event_queue.put(('answer', content))
            event_queue.put(('done', None))
        except Exception as e:
            event_queue.put(('error', str(e)))
    
    async def generate():
        thread = threading.Thread(target=run_agent, daemon=True)
        thread.start()
        
        while True:
            try:
                event_type, data = event_queue.get(timeout=0.1)
                
                if event_type == 'done':
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    break
                elif event_type == 'error':
                    yield f"data: {json.dumps({'type': 'error', 'content': data})}\n\n"
                    break
                elif event_type == 'log':
                    yield f"data: {json.dumps({'type': 'thinking', 'content': data})}\n\n"
                else:
                    yield f"data: {json.dumps({'type': event_type, 'content': data})}\n\n"
            except queue.Empty:
                await asyncio.sleep(0.1)
        
        thread.join(timeout=1)
    
    return StreamingResponse(generate(), media_type="text/event-stream")

def main():
    print("ğŸ“Š Starting Data Analysis Agent...")
    print("  - A2A Server on port 9003")
    print("  - HTTP API on port 9004")
    
    # Start A2A Server in background thread
    import threading
    a2a_server = A2AServer(agent=agent, port=9003)
    a2a_thread = threading.Thread(target=a2a_server.serve, daemon=True)
    a2a_thread.start()
    
    # Start FastAPI server
    uvicorn.run(app, host="127.0.0.1", port=9004)

if __name__ == "__main__":
    main()
